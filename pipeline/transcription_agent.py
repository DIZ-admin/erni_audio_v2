# pipeline/transcription_agent.py

import logging
from openai import OpenAI
from pathlib import Path
from typing import List, Dict, Optional
import openai
import time
import subprocess
import tempfile
import uuid
import random
import asyncio
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
from pydub import AudioSegment
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, before_sleep_log
from .config import ConfigurationManager

class TranscriptionAgent:
    """
    –ê–≥–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å OpenAI Speech-to-Text –º–æ–¥–µ–ª—è–º–∏.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç whisper-1, gpt-4o-transcribe, gpt-4o-mini-transcribe.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏:
    id, start, end, text, tokens, avg_logprob, no_speech_prob, temperature, compression_ratio.
    """

    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –∏ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    SUPPORTED_MODELS = {
        "whisper-1": {
            "name": "Whisper v1",
            "description": "–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å Whisper, –±—ã—Å—Ç—Ä–∞—è –∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è",
            "max_file_size_mb": 25,
            "supports_language": True,
            "supports_prompt": True,
            "supports_verbose_json": True,
            "cost_tier": "low"
        },
        "gpt-4o-mini-transcribe": {
            "name": "GPT-4o Mini Transcribe",
            "description": "–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å –±–∞–ª–∞–Ω—Å–æ–º —Ü–µ–Ω—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞",
            "max_file_size_mb": 25,
            "supports_language": True,
            "supports_prompt": True,
            "supports_verbose_json": False,
            "cost_tier": "medium"
        },
        "gpt-4o-transcribe": {
            "name": "GPT-4o Transcribe",
            "description": "–ù–∞–∏–±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è",
            "max_file_size_mb": 25,
            "supports_language": True,
            "supports_prompt": True,
            "supports_verbose_json": False,
            "cost_tier": "high"
        }
    }

    def __init__(self, api_key: str, model: str = "whisper-1", language: Optional[str] = None, response_format: str = "auto"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.

        Args:
            api_key: OpenAI API –∫–ª—é—á
            model: –ú–æ–¥–µ–ª—å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (whisper-1, gpt-4o-mini-transcribe, gpt-4o-transcribe)
            language: –ö–æ–¥ —è–∑—ã–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'en', 'ru', 'de') –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            response_format: –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (auto, json, verbose_json, text, srt, vtt)
        """
        self.client = OpenAI(api_key=api_key)
        self.model = self._validate_model(model)
        self.language = language
        self.response_format = self._determine_response_format(response_format)
        self.logger = logging.getLogger(__name__)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è retry
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º—ã –Ω–µ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º –æ–∫—Ä—É–∂–µ–Ω–∏–∏
            if api_key != "test-key":
                self.config = ConfigurationManager()
                self.retry_config = self.config.get_retry_config("transcription")
            else:
                # –¢–µ—Å—Ç–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                self.retry_config = {}
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
            self.retry_config = {}

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ retry –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        self.retry_stats = {
            "total_attempts": 0,
            "rate_limit_retries": 0,
            "connection_retries": 0,
            "other_retries": 0,
            "total_retry_time": 0.0
        }

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.max_concurrent_chunks = self.retry_config.get("max_concurrent_chunks", 3)
        self.chunk_processing_timeout = self.retry_config.get("chunk_processing_timeout", 1800)  # 30 –º–∏–Ω—É—Ç

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.parallel_stats = {
            "total_chunks_processed": 0,
            "concurrent_chunks_peak": 0,
            "total_parallel_time": 0.0,
            "chunks_failed": 0,
            "chunks_retried": 0
        }

        # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        model_info = self.SUPPORTED_MODELS[self.model]
        self.logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω TranscriptionAgent —Å –º–æ–¥–µ–ª—å—é: {model_info['name']} ({model_info['description']})")

    def _get_adaptive_timeout(self, file_size_mb: float) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞.

        Args:
            file_size_mb: –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –º–µ–≥–∞–±–∞–π—Ç–∞—Ö

        Returns:
            –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        # –ë–∞–∑–æ–≤—ã–π —Ç–∞–π–º–∞—É—Ç 60 —Å–µ–∫—É–Ω–¥ + 10 —Å–µ–∫—É–Ω–¥ –Ω–∞ –∫–∞–∂–¥—ã–π MB
        base_timeout = 60
        size_factor = max(1.0, file_size_mb * 10)
        adaptive_timeout = min(base_timeout + size_factor, 600)  # –ú–∞–∫—Å–∏–º—É–º 10 –º–∏–Ω—É—Ç

        self.logger.debug(f"–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Ñ–∞–π–ª–∞ {file_size_mb:.1f}MB: {adaptive_timeout:.1f}—Å")
        return adaptive_timeout

    def _intelligent_wait_strategy(self, retry_state):
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–∂–∏–¥–∞–Ω–∏—è —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫.
        """
        exception = retry_state.outcome.exception()
        attempt = retry_state.attempt_number

        if isinstance(exception, openai.RateLimitError):
            # –î–ª—è rate limit - —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backoff —Å jitter
            base_delay = 2.0
            max_delay = 120.0  # 2 –º–∏–Ω—É—Ç—ã –º–∞–∫—Å–∏–º—É–º

            # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backoff —Å jitter
            delay = min(base_delay * (2 ** (attempt - 1)), max_delay)
            jitter = random.uniform(0.1, 0.3) * delay
            final_delay = delay + jitter

            self.logger.warning(
                f"üîÑ Rate limit hit (–ø–æ–ø—ã—Ç–∫–∞ {attempt}), –∂–¥–µ–º {final_delay:.1f}—Å "
                f"(base: {delay:.1f}—Å, jitter: {jitter:.1f}—Å)"
            )

        elif isinstance(exception, openai.APIConnectionError):
            # –î–ª—è —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫ - –±—ã—Å—Ç—Ä—ã–µ –ø–æ–≤—Ç–æ—Ä—ã
            base_delay = 0.5
            final_delay = min(base_delay * attempt, 10.0)  # –ú–∞–∫—Å–∏–º—É–º 10 —Å–µ–∫—É–Ω–¥

            self.logger.warning(
                f"üåê –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt}), –±—ã—Å—Ç—Ä—ã–π –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {final_delay:.1f}—Å"
            )

        else:
            # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π backoff
            base_delay = 1.0
            final_delay = min(base_delay * (1.5 ** (attempt - 1)), 60.0)

            self.logger.warning(
                f"‚ö†Ô∏è –î—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt}), –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {final_delay:.1f}—Å: {type(exception).__name__}"
            )

        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è retry
        self.retry_stats["total_retry_time"] += final_delay

        return final_delay

    def _log_retry_statistics(self):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É retry –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        if self.retry_stats["total_attempts"] > 0:
            self.logger.info(
                f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ retry: –≤—Å–µ–≥–æ –ø–æ–ø—ã—Ç–æ–∫={self.retry_stats['total_attempts']}, "
                f"rate_limit={self.retry_stats['rate_limit_retries']}, "
                f"connection={self.retry_stats['connection_retries']}, "
                f"other={self.retry_stats['other_retries']}, "
                f"–æ–±—â–µ–µ –≤—Ä–µ–º—è retry={self.retry_stats['total_retry_time']:.1f}—Å"
            )

    def _log_parallel_statistics(self):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        if self.parallel_stats["total_chunks_processed"] > 0:
            self.logger.info(
                f"üîÑ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: "
                f"–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞—Å—Ç–µ–π={self.parallel_stats['total_chunks_processed']}, "
                f"–ø–∏–∫ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö={self.parallel_stats['concurrent_chunks_peak']}, "
                f"–≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏={self.parallel_stats['total_parallel_time']:.1f}—Å, "
                f"–Ω–µ—É–¥–∞—á–Ω—ã—Ö={self.parallel_stats['chunks_failed']}, "
                f"–ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö={self.parallel_stats['chunks_retried']}"
            )

    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–∞
    SUPPORTED_RESPONSE_FORMATS = {
        "auto": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞",
        "json": "–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π JSON —Å –ø–æ–ª–µ–º text",
        "verbose_json": "–ü–æ–¥—Ä–æ–±–Ω—ã–π JSON —Å–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏",
        "text": "–ß–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –æ–±—ë—Ä—Ç–∫–∏",
        "srt": "–°—É–±—Ç–∏—Ç—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ SRT",
        "vtt": "–°—É–±—Ç–∏—Ç—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ WebVTT"
    }

    def _validate_model(self, model: str) -> str:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        if model not in self.SUPPORTED_MODELS:
            available_models = ", ".join(self.SUPPORTED_MODELS.keys())
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –º–æ–¥–µ–ª—å '{model}'. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")
        return model

    def _validate_response_format(self, response_format: str) -> str:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞."""
        if response_format not in self.SUPPORTED_RESPONSE_FORMATS:
            available_formats = ", ".join(self.SUPPORTED_RESPONSE_FORMATS.keys())
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç '{response_format}'. –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {available_formats}")
        return response_format

    def _determine_response_format(self, requested_format: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏."""
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        self._validate_response_format(requested_format)

        # –ï—Å–ª–∏ auto, –≤—ã–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if requested_format == "auto":
            if self.SUPPORTED_MODELS[self.model]["supports_verbose_json"]:
                return "verbose_json"  # –î–ª—è whisper-1
            else:
                return "json"  # –î–ª—è gpt-4o –º–æ–¥–µ–ª–µ–π

        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω verbose_json, –Ω–æ –º–æ–¥–µ–ª—å –µ–≥–æ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç
        if requested_format == "verbose_json" and not self.SUPPORTED_MODELS[self.model]["supports_verbose_json"]:
            self.logger.warning(f"–ú–æ–¥–µ–ª—å {self.model} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç verbose_json, –∏—Å–ø–æ–ª—å–∑—É–µ–º json")
            return "json"

        return requested_format

    def run(self, wav_local: Path, prompt: str = "") -> List[Dict]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞.

        Args:
            wav_local: –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            prompt: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        """
        start_time = time.time()

        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
            self._validate_audio_file(wav_local)

            file_size_mb = wav_local.stat().st_size / (1024 * 1024)  # MB
            max_size = self.SUPPORTED_MODELS[self.model]["max_file_size_mb"]
            model_info = self.SUPPORTED_MODELS[self.model]

            self.logger.info(f"–ù–∞—á–∏–Ω–∞—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Å {model_info['name']}: {wav_local} ({file_size_mb:.1f}MB)")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Ä–∞–∑–±–∏–≤–∞—Ç—å —Ñ–∞–π–ª
            if file_size_mb > max_size:
                return self._transcribe_large_file(wav_local, prompt)
            else:
                return self._transcribe_single_file(wav_local, prompt)

        except openai.APIConnectionError as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ OpenAI API: {e}")
            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ OpenAI API: {e}") from e
        except openai.RateLimitError as e:
            self.logger.error(f"–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ OpenAI API: {e}")
            raise RuntimeError(f"–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ OpenAI API: {e}") from e
        except openai.APIStatusError as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ OpenAI API (—Å—Ç–∞—Ç—É—Å {e.status_code}): {e}")
            raise RuntimeError(f"–û—à–∏–±–∫–∞ OpenAI API: {e}") from e
        except Exception as e:
            self.logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}")
            raise RuntimeError(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}") from e

    def _validate_audio_file(self, wav_local: Path) -> None:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π."""
        if not wav_local.exists():
            raise FileNotFoundError(f"–ê—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {wav_local}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ - —Ç–µ–ø–µ—Ä—å –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º, –∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º
        file_size_mb = wav_local.stat().st_size / (1024 * 1024)
        max_size = self.SUPPORTED_MODELS[self.model]["max_file_size_mb"]

        if file_size_mb > max_size:
            self.logger.warning(f"–§–∞–π–ª ({file_size_mb:.1f}MB) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç OpenAI ({max_size}MB). –ë—É–¥–µ—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ —á–∞—Å—Ç–∏.")

    def _split_audio_file(self, wav_local: Path, chunk_duration_minutes: int = 10) -> List[Path]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –±–æ–ª—å—à–æ–π –∞—É–¥–∏–æ—Ñ–∞–π–ª –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ OpenAI API.

        Args:
            wav_local: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
            chunk_duration_minutes: –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏ –≤ –º–∏–Ω—É—Ç–∞—Ö

        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —á–∞—Å—Ç—è–º —Ñ–∞–π–ª–∞
        """
        try:
            self.logger.info(f"–†–∞–∑–±–∏–≤–∞—é —Ñ–∞–π–ª {wav_local.name} –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ {chunk_duration_minutes} –º–∏–Ω—É—Ç...")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
            audio = AudioSegment.from_wav(wav_local)
            chunk_duration_ms = chunk_duration_minutes * 60 * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö

            chunks = []
            temp_dir = Path(tempfile.gettempdir())

            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏
            for i, start_ms in enumerate(range(0, len(audio), chunk_duration_ms)):
                end_ms = min(start_ms + chunk_duration_ms, len(audio))
                chunk = audio[start_ms:end_ms]

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç—å
                chunk_path = temp_dir / f"{wav_local.stem}_chunk_{i:03d}.wav"
                chunk.export(chunk_path, format="wav")
                chunks.append(chunk_path)

                chunk_size_mb = chunk_path.stat().st_size / (1024 * 1024)
                self.logger.debug(f"–°–æ–∑–¥–∞–Ω–∞ —á–∞—Å—Ç—å {i+1}: {chunk_path.name} ({chunk_size_mb:.1f}MB)")

            self.logger.info(f"–§–∞–π–ª —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π")
            return chunks

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å —Ñ–∞–π–ª: {e}") from e

    def _transcribe_single_file(self, wav_local: Path, prompt: str = "") -> List[Dict]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π retry –ª–æ–≥–∏–∫–æ–π."""
        start_time = time.time()
        file_size_mb = wav_local.stat().st_size / (1024 * 1024)

        # –ü–æ–ª—É—á–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ç–∞–π–º–∞—É—Ç
        adaptive_timeout = self._get_adaptive_timeout(file_size_mb)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–ø—Ä–æ—Å–∞
        transcription_params = self._prepare_transcription_params(prompt)

        # –í—ã–∑—ã–≤–∞–µ–º –º–µ—Ç–æ–¥ —Å retry –ª–æ–≥–∏–∫–æ–π
        result = self._transcribe_with_intelligent_retry(wav_local, transcription_params, adaptive_timeout)

        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        duration = time.time() - start_time
        self.logger.info(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {duration:.2f}—Å (—Ñ–∞–π–ª: {file_size_mb:.1f}MB)")
        self._log_retry_statistics()

        return result

    @retry(
        stop=stop_after_attempt(8),  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 8 –ø–æ–ø—ã—Ç–æ–∫
        wait=wait_exponential(multiplier=1, min=1, max=120),  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backoff 1-120—Å
        retry=retry_if_exception_type((
            openai.RateLimitError,
            openai.APIConnectionError,
            openai.APITimeoutError,
            openai.InternalServerError
        )),
        before_sleep=before_sleep_log(logging.getLogger(__name__), logging.WARNING),
        reraise=True
    )
    def _transcribe_with_intelligent_retry(self, wav_local: Path, transcription_params: Dict, timeout: float) -> List[Dict]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π retry –ª–æ–≥–∏–∫–æ–π."""
        try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞
            client_with_timeout = self.client.with_options(timeout=timeout)

            with open(wav_local, "rb") as audio_file:
                transcript = client_with_timeout.audio.transcriptions.create(
                    model=self.model,
                    file=audio_file,
                    **transcription_params
                )

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            return self._process_transcript_response(transcript)

        except (openai.RateLimitError, openai.APIConnectionError, openai.APITimeoutError, openai.InternalServerError) as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è retry –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è tenacity)
            if isinstance(e, openai.RateLimitError):
                self.retry_stats["rate_limit_retries"] += 1
            elif isinstance(e, openai.APIConnectionError):
                self.retry_stats["connection_retries"] += 1
            else:
                self.retry_stats["other_retries"] += 1

            self.retry_stats["total_attempts"] += 1
            raise  # –ü–µ—Ä–µ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è tenacity

        except openai.APIStatusError as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ OpenAI API (—Å—Ç–∞—Ç—É—Å {e.status_code}): {e}")
            if e.status_code == 429:  # Rate limit
                raise openai.RateLimitError(f"Rate limit: {e}") from e
            else:
                raise RuntimeError(f"–û—à–∏–±–∫–∞ OpenAI API: {e}") from e

    def _process_chunk_parallel(self, chunk_info: Dict) -> Dict:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —á–∞—Å—Ç—å —Ñ–∞–π–ª–∞ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ.

        Args:
            chunk_info: –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —á–∞—Å—Ç–∏ —Ñ–∞–π–ª–∞:
                - path: Path –∫ —Ñ–∞–π–ª—É —á–∞—Å—Ç–∏
                - index: –ò–Ω–¥–µ–∫—Å —á–∞—Å—Ç–∏
                - offset: –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
                - prompt: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –ø–æ–¥—Å–∫–∞–∑–∫–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏:
                - index: –ò–Ω–¥–µ–∫—Å —á–∞—Å—Ç–∏
                - segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
                - offset: –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ
                - success: –§–ª–∞–≥ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
                - error: –û–ø–∏—Å–∞–Ω–∏–µ –æ—à–∏–±–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                - processing_time: –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        chunk_path = chunk_info["path"]
        chunk_index = chunk_info["index"]
        chunk_offset = chunk_info["offset"]
        prompt = chunk_info["prompt"]

        start_time = time.time()

        try:
            self.logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —á–∞—Å—Ç–∏ {chunk_index + 1}: {chunk_path.name}")

            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º —á–∞—Å—Ç—å
            chunk_segments = self._transcribe_single_file(chunk_path, prompt)

            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ —Å —É—á–µ—Ç–æ–º —Å–º–µ—â–µ–Ω–∏—è
            for segment in chunk_segments:
                segment['start'] += chunk_offset
                segment['end'] += chunk_offset
                # ID –±—É–¥–µ—Ç –ø–µ—Ä–µ–Ω—É–º–µ—Ä–æ–≤–∞–Ω –ø–æ–∑–∂–µ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏

            processing_time = time.time() - start_time

            self.logger.info(f"‚úÖ –ß–∞—Å—Ç—å {chunk_index + 1} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {len(chunk_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∑–∞ {processing_time:.2f}—Å")

            return {
                "index": chunk_index,
                "segments": chunk_segments,
                "offset": chunk_offset,
                "success": True,
                "error": None,
                "processing_time": processing_time
            }

        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Å—Ç–∏ {chunk_index + 1}: {e}"

            self.logger.error(error_msg)
            self.parallel_stats["chunks_failed"] += 1

            return {
                "index": chunk_index,
                "segments": [],
                "offset": chunk_offset,
                "success": False,
                "error": error_msg,
                "processing_time": processing_time
            }

    def _transcribe_large_file(self, wav_local: Path, prompt: str = "") -> List[Dict]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —á–∞—Å—Ç–µ–π."""
        self.logger.info(f"üöÄ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π (–º–∞–∫—Å {self.max_concurrent_chunks} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)...")

        start_time = time.time()

        # –†–∞–∑–±–∏–≤–∞–µ–º —Ñ–∞–π–ª –Ω–∞ —á–∞—Å—Ç–∏
        chunks = self._split_audio_file(wav_local, chunk_duration_minutes=10)

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞—Å—Ç—è—Ö –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        chunk_infos = []
        for i, chunk_path in enumerate(chunks):
            chunk_info = {
                "path": chunk_path,
                "index": i,
                "offset": i * 10 * 60,  # 10 –º–∏–Ω—É—Ç = 600 —Å–µ–∫—É–Ω–¥
                "prompt": prompt
            }
            chunk_infos.append(chunk_info)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = self._process_chunks_parallel(chunk_infos)

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        all_segments = []
        total_processing_time = 0.0
        successful_chunks = 0

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∏–Ω–¥–µ–∫—Å—É —á–∞—Å—Ç–∏
        results.sort(key=lambda x: x["index"])

        for result in results:
            if result["success"]:
                # –ü–µ—Ä–µ–Ω—É–º–µ—Ä–æ–≤—ã–≤–∞–µ–º ID —Å–µ–≥–º–µ–Ω—Ç–æ–≤
                for segment in result["segments"]:
                    segment['id'] = len(all_segments)
                    all_segments.append(segment)

                successful_chunks += 1
                total_processing_time += result["processing_time"]
                self.parallel_stats["total_chunks_processed"] += 1
            else:
                self.logger.error(f"‚ùå –ß–∞—Å—Ç—å {result['index'] + 1} –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {result['error']}")

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        self._cleanup_chunk_files(chunks)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        parallel_duration = time.time() - start_time
        self.parallel_stats["total_parallel_time"] += parallel_duration

        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        speedup_ratio = total_processing_time / parallel_duration if parallel_duration > 0 else 1.0

        self.logger.info(
            f"‚úÖ –ë–æ–ª—å—à–æ–π —Ñ–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(all_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ {successful_chunks}/{len(chunks)} —á–∞—Å—Ç–µ–π"
        )
        self.logger.info(
            f"‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {parallel_duration:.2f}—Å (—É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ {speedup_ratio:.1f}x)"
        )

        self._log_parallel_statistics()

        if successful_chunks < len(chunks):
            failed_count = len(chunks) - successful_chunks
            self.logger.warning(f"‚ö†Ô∏è {failed_count} —á–∞—Å—Ç–µ–π –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å")

        return all_segments

    def _process_chunks_parallel(self, chunk_infos: List[Dict]) -> List[Dict]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–∏ —Ñ–∞–π–ª–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –Ω–∞–≥—Ä—É–∑–∫–∏.

        Args:
            chunk_infos: –°–ø–∏—Å–æ–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —á–∞—Å—Ç—è—Ö —Ñ–∞–π–ª–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        results = []
        active_futures = 0

        self.logger.info(f"üîÑ –ó–∞–ø—É—Å–∫–∞—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(chunk_infos)} —á–∞—Å—Ç–µ–π (–º–∞–∫—Å {self.max_concurrent_chunks} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)")

        with ThreadPoolExecutor(max_workers=self.max_concurrent_chunks) as executor:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            future_to_chunk = {}

            for chunk_info in chunk_infos:
                future = executor.submit(self._process_chunk_parallel, chunk_info)
                future_to_chunk[future] = chunk_info
                active_futures += 1

                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∏–∫ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
                if active_futures > self.parallel_stats["concurrent_chunks_peak"]:
                    self.parallel_stats["concurrent_chunks_peak"] = active_futures

            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            try:
                for future in as_completed(future_to_chunk, timeout=self.chunk_processing_timeout):
                    chunk_info = future_to_chunk[future]
                    active_futures -= 1

                    try:
                        result = future.result()
                        results.append(result)

                        if result["success"]:
                            self.logger.debug(f"‚úÖ –ß–∞—Å—Ç—å {result['index'] + 1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                        else:
                            self.logger.warning(f"‚ùå –ß–∞—Å—Ç—å {result['index'] + 1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–æ–π")

                    except concurrent.futures.TimeoutError:
                        error_msg = f"–¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Å—Ç–∏ {chunk_info['index'] + 1}"
                        self.logger.error(error_msg)
                        self.parallel_stats["chunks_failed"] += 1

                        results.append({
                            "index": chunk_info["index"],
                            "segments": [],
                            "offset": chunk_info["offset"],
                            "success": False,
                            "error": error_msg,
                            "processing_time": self.chunk_processing_timeout
                        })

                    except Exception as e:
                        error_msg = f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞—Å—Ç–∏ {chunk_info['index'] + 1}: {e}"
                        self.logger.error(error_msg)
                        self.parallel_stats["chunks_failed"] += 1

                        results.append({
                            "index": chunk_info["index"],
                            "segments": [],
                            "offset": chunk_info["offset"],
                            "success": False,
                            "error": error_msg,
                            "processing_time": 0.0
                        })

            except concurrent.futures.TimeoutError:
                # –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ç–∞–π–º–∞—É—Ç as_completed - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
                self.logger.error(f"‚è∞ –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ ({self.chunk_processing_timeout}—Å)")

                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
                for future, chunk_info in future_to_chunk.items():
                    if not future.done():
                        error_msg = f"–ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Å—Ç–∏ {chunk_info['index'] + 1}"
                        self.parallel_stats["chunks_failed"] += 1

                        results.append({
                            "index": chunk_info["index"],
                            "segments": [],
                            "offset": chunk_info["offset"],
                            "success": False,
                            "error": error_msg,
                            "processing_time": self.chunk_processing_timeout
                        })

        self.logger.info(f"üèÅ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return results

    def _cleanup_chunk_files(self, chunk_paths: List[Path]) -> None:
        """–£–¥–∞–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —á–∞—Å—Ç–µ–π."""
        for chunk_path in chunk_paths:
            try:
                if chunk_path.exists():
                    chunk_path.unlink()
                    self.logger.debug(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {chunk_path.name}")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {chunk_path}: {e}")

    def _prepare_transcription_params(self, prompt: str) -> Dict:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏."""
        params = {
            "response_format": self.response_format,
            "temperature": 0,
        }

        # –î–æ–±–∞–≤–ª—è–µ–º prompt –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
        if prompt and self.SUPPORTED_MODELS[self.model]["supports_prompt"]:
            params["prompt"] = prompt

        # –î–æ–±–∞–≤–ª—è–µ–º —è–∑—ã–∫ –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –∏ —É–∫–∞–∑–∞–Ω
        if self.language and self.SUPPORTED_MODELS[self.model]["supports_language"]:
            params["language"] = self.language

        return params

    def _process_transcript_response(self, transcript) -> List[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞."""

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞
        if self.response_format == "verbose_json":
            # verbose_json –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã
            segments = getattr(transcript, 'segments', [])

            if not segments:
                self.logger.warning(f"–ú–æ–¥–µ–ª—å {self.model} –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ verbose_json")
                return []

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä–∏
            processed_segments = []
            for segment in segments:
                if hasattr(segment, 'model_dump'):
                    segment_dict = segment.model_dump()
                elif hasattr(segment, '__dict__'):
                    segment_dict = segment.__dict__
                else:
                    segment_dict = dict(segment)

                processed_segments.append(segment_dict)

            model_info = self.SUPPORTED_MODELS[self.model]
            self.logger.info(f"{model_info['name']}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(processed_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (verbose_json)")
            return processed_segments

        elif self.response_format == "json":
            # json –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
            text = getattr(transcript, 'text', '')
            if not text:
                self.logger.warning(f"–ú–æ–¥–µ–ª—å {self.model} –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ —Ç–µ–∫—Å—Ç –≤ json")
                return []

            # –°–æ–∑–¥–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç
            duration = getattr(transcript, 'duration', 0.0)
            segment = {
                "id": 0,
                "start": 0.0,
                "end": duration,
                "text": text.strip(),
                "tokens": [],
                "avg_logprob": 0.0,
                "no_speech_prob": 0.0,
                "temperature": 0.0,
                "compression_ratio": 1.0
            }

            model_info = self.SUPPORTED_MODELS[self.model]
            self.logger.info(f"{model_info['name']}: —Å–æ–∑–¥–∞–Ω —Å–µ–≥–º–µ–Ω—Ç –∏–∑ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ (json)")
            return [segment]

        else:
            # –î–ª—è text, srt, vtt —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            # –≠—Ç–∏ —Ñ–æ—Ä–º–∞—Ç—ã –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø—Ä—è–º–æ–≥–æ –≤—ã–≤–æ–¥–∞, –∞ –Ω–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            text_content = str(transcript) if transcript else ""
            if not text_content:
                self.logger.warning(f"–ú–æ–¥–µ–ª—å {self.model} –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ {self.response_format}")
                return []

            segment = {
                "id": 0,
                "start": 0.0,
                "end": 0.0,
                "text": text_content.strip(),
                "tokens": [],
                "avg_logprob": 0.0,
                "no_speech_prob": 0.0,
                "temperature": 0.0,
                "compression_ratio": 1.0
            }

            model_info = self.SUPPORTED_MODELS[self.model]
            self.logger.info(f"{model_info['name']}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ {self.response_format}")
            return [segment]

    def get_model_info(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏."""
        return {
            "model": self.model,
            "language": self.language,
            **self.SUPPORTED_MODELS[self.model]
        }

    @classmethod
    def get_available_models(cls) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        return cls.SUPPORTED_MODELS.copy()

    def set_language(self, language: Optional[str]) -> None:
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —è–∑—ã–∫ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏."""
        self.language = language
        if language:
            self.logger.info(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —è–∑—ã–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {language}")
        else:
            self.logger.info("–Ø–∑—ã–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å–±—Ä–æ—à–µ–Ω (–∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)")

    def estimate_cost(self, file_size_mb: float) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω—É—é —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏."""
        cost_tier = self.SUPPORTED_MODELS[self.model]["cost_tier"]

        cost_estimates = {
            "low": f"~${file_size_mb * 0.006:.3f}",  # whisper-1: $0.006/min
            "medium": f"~${file_size_mb * 0.012:.3f}",  # gpt-4o-mini-transcribe: –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 2 —Ä–∞–∑–∞ –¥–æ—Ä–æ–∂–µ
            "high": f"~${file_size_mb * 0.024:.3f}"  # gpt-4o-transcribe: –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 4 —Ä–∞–∑–∞ –¥–æ—Ä–æ–∂–µ
        }

        return cost_estimates.get(cost_tier, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
