"""
Transcription Quality Tester - –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç WER/CER –º–µ—Ç—Ä–∏–∫–∏
"""

import logging
import time
import os
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import json
from dataclasses import dataclass

from .transcription_agent import TranscriptionAgent
from .replicate_agent import ReplicateAgent
from .wer_evaluator import WERTranscriptionEvaluator, TranscriptionResult, WERCalculator


@dataclass
class TestScenario:
    """–¢–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
    name: str
    audio_file: Path
    reference_text: str
    description: str
    language: Optional[str] = None
    expected_speakers: Optional[int] = None


class TranscriptionQualityTester:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏"""
    
    def __init__(self, openai_api_key: str, replicate_api_key: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–µ—Ä–∞
        
        Args:
            openai_api_key: OpenAI API –∫–ª—é—á
            replicate_api_key: Replicate API –∫–ª—é—á (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.openai_api_key = openai_api_key
        self.replicate_api_key = replicate_api_key
        self.logger = logging.getLogger(__name__)
        self.evaluator = WERTranscriptionEvaluator()
        
        # –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.openai_models = list(TranscriptionAgent.SUPPORTED_MODELS.keys())
        self.replicate_available = replicate_api_key is not None
        
        self.logger.info(f"üß™ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω TranscriptionQualityTester")
        self.logger.info(f"üìã OpenAI –º–æ–¥–µ–ª–∏: {', '.join(self.openai_models)}")
        self.logger.info(f"üöÄ Replicate –¥–æ—Å—Ç—É–ø–µ–Ω: {'–î–∞' if self.replicate_available else '–ù–µ—Ç'}")
    
    def create_test_scenarios(self) -> List[TestScenario]:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
        """
        scenarios = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª—ã
        test_files = [
            ("data/raw/Sitzung_GL_converted.mp3", "–ù–µ–º–µ—Ü–∫–∞—è –¥–µ–ª–æ–≤–∞—è –≤—Å—Ç—Ä–µ—á–∞", "de"),
            ("data/raw/Sitzung Erweiterte GL 17.04.2025.m4a", "–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤—Å—Ç—Ä–µ—á–∞ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞", "de"),
        ]
        
        for file_path, description, language in test_files:
            audio_path = Path(file_path)
            if audio_path.exists():
                # –°–æ–∑–¥–∞–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                reference_text = self._generate_reference_text(audio_path, description)
                
                scenario = TestScenario(
                    name=audio_path.stem,
                    audio_file=audio_path,
                    reference_text=reference_text,
                    description=description,
                    language=language,
                    expected_speakers=2  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 2 —Å–ø–∏–∫–µ—Ä–∞ –¥–ª—è –¥–µ–ª–æ–≤—ã—Ö –≤—Å—Ç—Ä–µ—á
                )
                scenarios.append(scenario)
                self.logger.info(f"üìù –°–æ–∑–¥–∞–Ω —Å—Ü–µ–Ω–∞—Ä–∏–π: {scenario.name}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ assets –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
        if not scenarios:
            scenarios.extend(self._create_mock_scenarios())
        
        return scenarios
    
    def _generate_reference_text(self, audio_path: Path, description: str) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞
        
        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            description: –û–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            
        Returns:
            –≠—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≥–æ—Ç–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
        reference_file = audio_path.parent / f"{audio_path.stem}_reference.txt"
        if reference_file.exists():
            return reference_file.read_text(encoding='utf-8').strip()
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –≥–æ—Ç–æ–≤–æ–≥–æ —ç—Ç–∞–ª–æ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø–∏—Å–∞–Ω–∏—è
        if "–Ω–µ–º–µ—Ü–∫" in description.lower() or "deutsch" in description.lower():
            return ("Guten Tag, willkommen zur Sitzung. "
                   "Heute besprechen wir die wichtigsten Punkte unserer Agenda. "
                   "Lassen Sie uns mit dem ersten Thema beginnen.")
        else:
            return ("–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –Ω–∞ –≤—Å—Ç—Ä–µ—á—É. "
                   "–°–µ–≥–æ–¥–Ω—è –º—ã –æ–±—Å—É–¥–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–≤–µ—Å—Ç–∫–∏ –¥–Ω—è. "
                   "–î–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º —Å –ø–µ—Ä–≤–æ–≥–æ –ø—É–Ω–∫—Ç–∞.")
    
    def _create_mock_scenarios(self) -> List[TestScenario]:
        """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Å –º–æ–∫–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        mock_scenarios = []
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
        test_audio = Path("tests/assets/test_audio_mock.wav")
        if not test_audio.exists():
            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            test_audio.parent.mkdir(parents=True, exist_ok=True)
            test_audio.write_bytes(b"RIFF" + b"\x00" * 44)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π WAV –∑–∞–≥–æ–ª–æ–≤–æ–∫
        
        scenario = TestScenario(
            name="mock_test",
            audio_file=test_audio,
            reference_text="–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏.",
            description="–¢–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π —Å –º–æ–∫–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏",
            language="ru",
            expected_speakers=1
        )
        mock_scenarios.append(scenario)
        
        return mock_scenarios
    
    def test_openai_model(self, model: str, scenario: TestScenario) -> TranscriptionResult:
        """
        –¢–µ—Å—Ç–∏—Ä—É–µ—Ç OpenAI –º–æ–¥–µ–ª—å –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏
        
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            scenario: –¢–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"üîÑ –¢–µ—Å—Ç–∏—Ä—É—é {model} –Ω–∞ {scenario.name}...")
            
            agent = TranscriptionAgent(
                api_key=self.openai_api_key,
                model=model,
                language=scenario.language
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫—É —Å—Ç–æ–∏–º–æ—Å—Ç–∏
            file_size_mb = scenario.audio_file.stat().st_size / (1024 * 1024)
            estimated_cost = agent.estimate_cost(file_size_mb)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
            segments = agent.run(scenario.audio_file, "")
            
            processing_time = time.time() - start_time
            
            return TranscriptionResult(
                model_name=model,
                segments=segments,
                processing_time=processing_time,
                estimated_cost=estimated_cost,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è {model}: {e}")
            
            return TranscriptionResult(
                model_name=model,
                segments=[],
                processing_time=processing_time,
                estimated_cost="N/A",
                success=False,
                error=str(e)
            )
    
    def test_replicate_model(self, scenario: TestScenario) -> Optional[TranscriptionResult]:
        """
        –¢–µ—Å—Ç–∏—Ä—É–µ—Ç Replicate –º–æ–¥–µ–ª—å –Ω–∞ —Å—Ü–µ–Ω–∞—Ä–∏–∏
        
        Args:
            scenario: –¢–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏–ª–∏ None –µ—Å–ª–∏ Replicate –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        """
        if not self.replicate_available:
            return None
        
        start_time = time.time()
        
        try:
            self.logger.info(f"üöÄ –¢–µ—Å—Ç–∏—Ä—É—é Replicate –Ω–∞ {scenario.name}...")
            
            agent = ReplicateAgent(api_token=self.replicate_api_key)
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫—É —Å—Ç–æ–∏–º–æ—Å—Ç–∏
            cost_info = agent.estimate_cost(scenario.audio_file)
            estimated_cost = f"~${cost_info['estimated_cost_usd']}"
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
            segments = agent.run(
                audio_file=scenario.audio_file,
                num_speakers=scenario.expected_speakers,
                language=scenario.language
            )
            
            processing_time = time.time() - start_time
            
            return TranscriptionResult(
                model_name="replicate-whisper-diarization",
                segments=segments,
                processing_time=processing_time,
                estimated_cost=estimated_cost,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Replicate: {e}")
            
            return TranscriptionResult(
                model_name="replicate-whisper-diarization",
                segments=[],
                processing_time=processing_time,
                estimated_cost="N/A",
                success=False,
                error=str(e)
            )
    
    def run_comprehensive_test(self, scenarios: Optional[List[TestScenario]] = None) -> Dict:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            scenarios: –°–ø–∏—Å–æ–∫ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        if scenarios is None:
            scenarios = self.create_test_scenarios()
        
        if not scenarios:
            raise ValueError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤")
        
        self.logger.info(f"üß™ –ù–∞—á–∏–Ω–∞—é –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {len(scenarios)} —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤...")
        
        all_results = {
            "test_summary": {
                "total_scenarios": len(scenarios),
                "total_models": len(self.openai_models) + (1 if self.replicate_available else 0),
                "start_time": time.time()
            },
            "scenarios": {},
            "model_comparison": {}
        }
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π
        for scenario in scenarios:
            self.logger.info(f"üìã –¢–µ—Å—Ç–∏—Ä—É—é —Å—Ü–µ–Ω–∞—Ä–∏–π: {scenario.name}")
            
            scenario_results = {
                "scenario_info": {
                    "name": scenario.name,
                    "description": scenario.description,
                    "audio_file": str(scenario.audio_file),
                    "language": scenario.language,
                    "reference_text": scenario.reference_text
                },
                "model_results": {}
            }
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º OpenAI –º–æ–¥–µ–ª–∏
            for model in self.openai_models:
                result = self.test_openai_model(model, scenario)
                evaluation = self.evaluator.evaluate_transcription(scenario.reference_text, result)
                scenario_results["model_results"][model] = evaluation
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º Replicate –º–æ–¥–µ–ª—å
            if self.replicate_available:
                result = self.test_replicate_model(scenario)
                if result:
                    evaluation = self.evaluator.evaluate_transcription(scenario.reference_text, result)
                    scenario_results["model_results"]["replicate-whisper-diarization"] = evaluation
            
            all_results["scenarios"][scenario.name] = scenario_results
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É –ø–æ –º–æ–¥–µ–ª—è–º
        all_results["model_comparison"] = self._create_model_comparison(all_results["scenarios"])
        all_results["test_summary"]["end_time"] = time.time()
        all_results["test_summary"]["total_duration"] = all_results["test_summary"]["end_time"] - all_results["test_summary"]["start_time"]
        
        return all_results

    def _create_model_comparison(self, scenarios_results: Dict) -> Dict:
        """
        –°–æ–∑–¥–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –º–æ–¥–µ–ª–µ–π

        Args:
            scenarios_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ü–µ–Ω–∞—Ä–∏—è–º

        Returns:
            –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–æ–¥–µ–ª–µ–π
        """
        model_stats = {}

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for scenario_name, scenario_data in scenarios_results.items():
            for model_name, model_result in scenario_data["model_results"].items():
                if model_name not in model_stats:
                    model_stats[model_name] = {
                        "total_tests": 0,
                        "successful_tests": 0,
                        "total_wer": 0.0,
                        "total_cer": 0.0,
                        "total_processing_time": 0.0,
                        "costs": []
                    }

                stats = model_stats[model_name]
                stats["total_tests"] += 1

                if model_result.get("success", False):
                    stats["successful_tests"] += 1
                    quality_metrics = model_result.get("quality_metrics", {})
                    stats["total_wer"] += quality_metrics.get("wer", 1.0)
                    stats["total_cer"] += quality_metrics.get("cer", 1.0)
                    stats["total_processing_time"] += model_result.get("processing_time", 0.0)

                    # –ü–∞—Ä—Å–∏–º —Å—Ç–æ–∏–º–æ—Å—Ç—å
                    cost_str = model_result.get("estimated_cost", "N/A")
                    if cost_str.startswith("~$"):
                        try:
                            cost_value = float(cost_str.replace("~$", ""))
                            stats["costs"].append(cost_value)
                        except ValueError:
                            pass

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        comparison = {}
        for model_name, stats in model_stats.items():
            successful_tests = stats["successful_tests"]

            if successful_tests > 0:
                avg_wer = stats["total_wer"] / successful_tests
                avg_cer = stats["total_cer"] / successful_tests
                avg_processing_time = stats["total_processing_time"] / successful_tests
                avg_cost = sum(stats["costs"]) / len(stats["costs"]) if stats["costs"] else 0.0

                comparison[model_name] = {
                    "success_rate": successful_tests / stats["total_tests"],
                    "average_wer": round(avg_wer, 4),
                    "average_cer": round(avg_cer, 4),
                    "word_accuracy": round(1.0 - avg_wer, 4),
                    "char_accuracy": round(1.0 - avg_cer, 4),
                    "average_processing_time": round(avg_processing_time, 2),
                    "average_cost_usd": round(avg_cost, 4),
                    "total_tests": stats["total_tests"],
                    "successful_tests": successful_tests
                }
            else:
                comparison[model_name] = {
                    "success_rate": 0.0,
                    "average_wer": 1.0,
                    "average_cer": 1.0,
                    "word_accuracy": 0.0,
                    "char_accuracy": 0.0,
                    "average_processing_time": 0.0,
                    "average_cost_usd": 0.0,
                    "total_tests": stats["total_tests"],
                    "successful_tests": 0,
                    "note": "–í—Å–µ —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å –æ—à–∏–±–∫–æ–π"
                }

        return comparison

    def generate_report(self, results: Dict, output_dir: Path = None) -> Path:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏

        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞

        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –æ—Ç—á–µ—Ç—É
        """
        if output_dir is None:
            output_dir = Path("data/interim")

        output_dir.mkdir(parents=True, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        json_path = output_dir / "wer_evaluation_results.json"
        self.evaluator.save_results(results, json_path)

        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        report_path = output_dir / "transcription_quality_report.md"
        self._generate_markdown_report(results, report_path)

        self.logger.info(f"üìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        return report_path

    def _generate_markdown_report(self, results: Dict, output_path: Path) -> None:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Markdown –æ—Ç—á–µ—Ç"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# –û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏\n\n")

            # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            summary = results["test_summary"]
            f.write(f"**–î–∞—Ç–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤:** {summary['total_scenarios']}\n")
            f.write(f"**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π:** {summary['total_models']}\n")
            f.write(f"**–û–±—â–µ–µ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:** {summary['total_duration']:.2f} —Å–µ–∫—É–Ω–¥\n\n")

            # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
            f.write("## –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n\n")
            f.write("| –ú–æ–¥–µ–ª—å | –¢–æ—á–Ω–æ—Å—Ç—å —Å–ª–æ–≤ | –¢–æ—á–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤ | WER | CER | –í—Ä–µ–º—è (—Å) | –°—Ç–æ–∏–º–æ—Å—Ç—å ($) | –£—Å–ø–µ—à–Ω–æ—Å—Ç—å |\n")
            f.write("|--------|---------------|-------------------|-----|-----|-----------|---------------|------------|\n")

            comparison = results["model_comparison"]
            for model_name, stats in sorted(comparison.items(), key=lambda x: x[1]["word_accuracy"], reverse=True):
                f.write(f"| {model_name} | {stats['word_accuracy']:.3f} | {stats['char_accuracy']:.3f} | "
                       f"{stats['average_wer']:.3f} | {stats['average_cer']:.3f} | "
                       f"{stats['average_processing_time']:.2f} | {stats['average_cost_usd']:.4f} | "
                       f"{stats['success_rate']:.1%} |\n")

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            f.write("\n## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n\n")
            best_accuracy = max(comparison.values(), key=lambda x: x["word_accuracy"])
            fastest = min([v for v in comparison.values() if v["success_rate"] > 0], key=lambda x: x["average_processing_time"])
            cheapest = min([v for v in comparison.values() if v["success_rate"] > 0], key=lambda x: x["average_cost_usd"])

            best_model = [k for k, v in comparison.items() if v == best_accuracy][0]
            fastest_model = [k for k, v in comparison.items() if v == fastest][0]
            cheapest_model = [k for k, v in comparison.items() if v == cheapest][0]

            f.write(f"- **–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å:** {best_model} (—Ç–æ—á–Ω–æ—Å—Ç—å —Å–ª–æ–≤: {best_accuracy['word_accuracy']:.3f})\n")
            f.write(f"- **–°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è:** {fastest_model} (–≤—Ä–µ–º—è: {fastest['average_processing_time']:.2f}—Å)\n")
            f.write(f"- **–°–∞–º–∞—è —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è:** {cheapest_model} (—Å—Ç–æ–∏–º–æ—Å—Ç—å: ${cheapest['average_cost_usd']:.4f})\n")

            # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ü–µ–Ω–∞—Ä–∏—è–º
            f.write("\n## –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n")
            for scenario_name, scenario_data in results["scenarios"].items():
                f.write(f"### {scenario_name}\n\n")
                f.write(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {scenario_data['scenario_info']['description']}\n")
                language = scenario_data['scenario_info'].get('language', '–ù–µ —É–∫–∞–∑–∞–Ω')
                f.write(f"**–Ø–∑—ã–∫:** {language}\n")
                f.write(f"**–≠—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:** {scenario_data['scenario_info']['reference_text'][:100]}...\n\n")

                for model_name, result in scenario_data["model_results"].items():
                    if result.get("success", False):
                        metrics = result["quality_metrics"]
                        f.write(f"- **{model_name}:** WER={metrics['wer']:.3f}, "
                               f"–≤—Ä–µ–º—è={result['processing_time']:.2f}—Å, "
                               f"—Å—Ç–æ–∏–º–æ—Å—Ç—å={result['estimated_cost']}\n")
                    else:
                        f.write(f"- **{model_name}:** ‚ùå –û—à–∏–±–∫–∞ - {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}\n")

                f.write("\n")
